{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVjU5375SP9/Na2veVnW5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashnaeldho/EVA7/blob/main/Assignment_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqzz7oWuKKr0"
      },
      "source": [
        "1. What are Channels and Kernels (according to EVA)?\n",
        "\n",
        "     The process of moving kernels on top of the input is called as convolution. When the input is convoluted with the kernel, we get channels. Kernels are used to extract features from the input. Kernels extract edges and gradients, patterns, part of an object and object. Channels contain these extracted features. Specific channel can detect a particular feature of different color, different size and orientation etc.\n",
        "\n",
        "2. Why should we (nearly) always use 3x3 kernels?\n",
        "\n",
        "     3x3 kernels are mostly used for convolution. Below mentioned are the reasons:\n",
        "     a)  It makes GPU's happier. It has something to do with optimization which NVIDIA has done.\n",
        "     b) It uses less number of parameters. For example, input image of size 5x5 can be converted to 1x1 by either using two 3x3 kernels or by using a 5x5 kernel. If we use two 3x3 kernels, the total number of parameters used will be 18 but if we use 5x5, then the total number of parameters used will be 25.\n",
        "    c) It has something to do with symmetry.\n",
        " \n",
        "3. How many times do we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)\n",
        "\n",
        "\n",
        "\n",
        "199x199\t>\t197x197\n",
        "\n",
        "197x197\t>\t195x195\n",
        "\n",
        "195x195\t>\t193x193\n",
        "\n",
        "193x193\t>\t191x191\n",
        "\n",
        "191x191\t>\t189x189\n",
        "\n",
        "189x189\t>\t187x187\n",
        "\n",
        "187x187\t>\t185x185\n",
        "\n",
        "185x185\t>\t183x183\n",
        "\n",
        "183x183\t>\t181x181\n",
        "\n",
        "181x181\t>\t179x179\n",
        "\n",
        "179x179\t>\t177x177\n",
        "\n",
        "177x177\t>\t175x175\n",
        "\n",
        "175x175\t>\t173x173\n",
        "\n",
        "173x173\t>\t171x171\n",
        "\n",
        "171x171\t>\t169x169\n",
        "\n",
        "169x169\t>\t167x167\n",
        "\n",
        "167x167\t>\t165x165\n",
        "\n",
        "165x165\t>\t163x163\n",
        "\n",
        "163x163\t>\t161x161\n",
        "\n",
        "161x161\t>\t159x159\n",
        "\n",
        "159x159\t>\t157x157\n",
        "\n",
        "157x157\t>\t155x155\n",
        "\n",
        "155x155\t>\t153x153\n",
        "\n",
        "153x153\t>\t151x151\n",
        "\n",
        "151x151\t>\t149x149\n",
        "\n",
        "149x149\t>\t147x147\n",
        "\n",
        "147x147\t>\t145x145\n",
        "\n",
        "145x145\t>\t143x143\n",
        "\n",
        "143x143\t>\t141x141\n",
        "\n",
        "141x141\t>\t139x139\n",
        "\n",
        "139x139\t>\t137x137\n",
        "\n",
        "137x137\t>\t135x135\n",
        "\n",
        "135x135\t>\t133x133\n",
        "\n",
        "133x133\t>\t131x131\n",
        "\n",
        "131x131\t>\t129x129\n",
        "\n",
        "129x129\t>\t127x127\n",
        "\n",
        "127x127\t>\t125x125\n",
        "\n",
        "125x125\t>\t123x123\n",
        "\n",
        "123x123\t>\t121x121\n",
        "\n",
        "121x121\t>\t119x119\n",
        "\n",
        "119x119\t>\t117x117\n",
        "\n",
        "117x117\t>\t115x115\n",
        "\n",
        "115x115\t>\t113x113\n",
        "\n",
        "113x113\t>\t111x111\n",
        "\n",
        "111x111\t>\t109x109\n",
        "\n",
        "109x109\t>\t107x107\n",
        "\n",
        "107x107\t>\t105x105\n",
        "\n",
        "105x105\t>\t103x103\n",
        "\n",
        "103x103\t>\t101x101\n",
        "\n",
        "101x101\t>\t99x99\n",
        "\n",
        "99x99\t>\t97x97\n",
        "\n",
        "97x97\t>\t95x95\n",
        "\n",
        "95x95\t>\t93x93\n",
        "\n",
        "93x93\t>\t91x91\n",
        "\n",
        "91x91\t>\t89x89\n",
        "\n",
        "89x89\t>\t87x87\n",
        "\n",
        "87x87\t>\t85x85\n",
        "\n",
        "85x85\t>\t83x83\n",
        "\n",
        "83x83\t>\t81x81\n",
        "\n",
        "81x81\t>\t79x79\n",
        "\n",
        "79x79\t>\t77x77\n",
        "\n",
        "77x77\t>\t75x75\n",
        "\n",
        "75x75\t>\t73x73\n",
        "\n",
        "73x73\t>\t71x71\n",
        "\n",
        "71x71\t>\t69x69\n",
        "\n",
        "69x69\t>\t67x67\n",
        "\n",
        "67x67\t>\t65x65\n",
        "\n",
        "65x65\t>\t63x63\n",
        "\n",
        "63x63\t>\t61x61\n",
        "\n",
        "61x61\t>\t59x59\n",
        "\n",
        "59x59\t>\t57x57\n",
        "\n",
        "57x57\t>\t55x55\n",
        "\n",
        "55x55\t>\t53x53\n",
        "\n",
        "53x53\t>\t51x51\n",
        "\n",
        "51x51\t>\t49x49\n",
        "\n",
        "49x49\t>\t47x47\n",
        "\n",
        "47x47\t>\t45x45\n",
        "\n",
        "45x45\t>\t43x43\n",
        "\n",
        "43x43\t>\t41x41\n",
        "\n",
        "41x41\t>\t39x39\n",
        "\n",
        "39x39\t>\t37x37\n",
        "\n",
        "37x37\t>\t35x35\n",
        "\n",
        "35x35\t>\t33x33\n",
        "\n",
        "33x33\t>\t31x31\n",
        "\n",
        "31x31\t>\t29x29\n",
        "\n",
        "29x29\t>\t27x27\n",
        "\n",
        "27x27\t>\t25x25\n",
        "\n",
        "25x25\t>\t23x23\n",
        "\n",
        "23x23\t>\t21x21\n",
        "\n",
        "21x21\t>\t19x19\n",
        "\n",
        "19x19\t>\t17x17\n",
        "\n",
        "17x17\t>\t15x15\n",
        "\n",
        "15x15\t>\t13x13\n",
        "\n",
        "13x13\t>\t11x11\n",
        "\n",
        "11x11\t>\t9x9\n",
        "\n",
        "9x9\t>\t7x7\n",
        "\n",
        "7x7\t>\t5x5\n",
        "\n",
        "5x5\t>\t3x3\n",
        "\n",
        "3x3\t>\t1x1\n",
        " \n",
        "4. How are kernels initialized?\n",
        "\n",
        " The process of moving kernels on top of the input is called as convolution. When kernels are convoluted with input, we get channels. Kernels extract features from the input. There are basically 4 blocks of convolution. In the first block, input image is convoluted with kernel to extract edges and gradients. Initially, Kernels are randomly initialized. Later they are fine tuned to extract the necessary features. In the second block, kernels extract patterns. Parts of objects and finally objects are extracted by kernels in the third and fourth block. When kernels are convoluted with the input, we get channels. Channels contain these extracted features. Specific channel can detect a particular feature of different color, different size and orientation etc.\n",
        " \n",
        "5. What happens during the training of a DNN?\n",
        "\n",
        " The process of moving kernels on top of the input is called as convolution. When kernels are convoluted with input, we get channels. Kernels extract features from the input. There are basically 4 blocks of convolution. In the first block, input image is convoluted with kernel to extract edges and gradients. Initially, Kernels are randomly initialized. Later they are fine tuned to extract the necessary features. In the second block, kernels extract patterns. Parts of objects and finally objects are extracted by kernels in the third and fourth block. When kernels are convoluted with the input, we get channels. Channels contain these extracted features. Specific channel can detect a particular feature of different color, different size and orientation etc. \n",
        "When we train a DNN, the neural network will do convolution of input with kernels to extract features. Fine tuning of kernel weights will happen during training so that the network can extract required information. We can specify the number of kernels used, size of the kernel and size of the channel, but the values of the kernels and what these kernel extract are decided by the network. While developing our network, we need to make sure that the final layer is able to see the entire image ,i.e, the global receptive field of the final output should be equal to the size of the input image. Receptive field is how many pixels we can directly or indirectly look at in an image.\n",
        "When a 5x5 input is convoluted with a 3x3 kernel, the output will be of size 3x3. Usually 3x3 kernels are preferred because it uses less number of parameters, has something to do with symmetry and has something to do with optimization which NVIDIA has done.\n"
      ]
    }
  ]
}